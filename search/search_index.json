{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Kurve","text":"<p>Self-serve demo available at https://demo.kurve.ai</p>"},{"location":"#deployment","title":"Deployment","text":"<p>Kurve is deployed into your environment behind your VPC so no data ever leaves your network.</p>"},{"location":"#metadata-extraction-and-inference","title":"Metadata extraction and inference","text":"<p>Kurve providers a number of algorithms that point at data catalogs and infers the relational structure. The metadata Kurve automatically extracts are the following:</p>"},{"location":"#per-table","title":"Per table","text":"<ol> <li>primary key</li> <li>date key</li> <li>row count</li> </ol>"},{"location":"#per-table-pair","title":"Per table pair","text":"<ol> <li>join keys (if any)</li> <li>cardinality</li> </ol>"},{"location":"#metadata-graphs","title":"Metadata graphs","text":"<p>Kurve metadata graphs use graph data structures, powered by networkx under the hood, to represent tables as nodes and relationships between tables as edges.  This allows us to benefit from the whole field of graph theory and take advantage of the rich ecosystem around it.</p>"},{"location":"#compute-graphs","title":"Compute graphs","text":"<p>Metadata graphs are combined with compute graphs to run multi-table data integration.  Since the metadata are repesented in a graph it allows us to perform depth first graph traversal based on cardinality to integrate and aggregate data in a bottom up way (start at the high row count fact tables and aggregate upward).</p>"},{"location":"#quickstart-demo","title":"Quickstart demo","text":"<ol> <li>Create an account on demo.kurve.ai</li> <li>Build a metadata graph on the sample data source <code>/usr/local/lake/cust_data</code>.</li> <li>this will infer the primary keys, date keys, and foreign keys between the tables</li> <li>Inspect the metadata graph to confirm the relationships are correct.</li> <li>Build a compute graph with the <code>cust.csv</code> as the parent node using the following parameters:<ol> <li>name: \"customer sample test\"</li> <li>parent node: <code>cust.csv</code></li> <li>depth limit: 2</li> <li>compute period in days: 365</li> <li>cut date: 5/1/2023</li> <li>label period in days: 60</li> <li>label node: <code>orders.csv</code></li> <li>label field: <code>id</code></li> <li>label operation: count</li> </ol> </li> <li>In the compute graph viewier click on Actions and then Execute</li> <li>Navigate to the home screen and you should see a data source under My data sources/</li> <li>Click on the data source and then click on the table name that was created which should    be the lower cased and underscore separated name you used for the compute graph.</li> </ol>"},{"location":"#what-just-happened","title":"What just happened?","text":"<ol> <li>The parent node indicates which table everything will be integrated to, in this case the <code>cust.csv</code>.</li> <li>The depth limit indicates how many joins away from the parent node are permissible, in this case tables within 2 joins are included.</li> <li>The cut date is the date around which to filter - so nothing after the cut date will be included in   the data aggregation / integration process.</li> <li>The compute period indicates how far backwards relative to the cut date to look in all of the tables. In this example this results in where clauses like this being added to each node in the graph:     <code>sql     where {table_date_key} &gt; {cut_date} - interval '365 day'</code></li> <li>The label period in days is how many days after the cut date to look for computing the label.   In this example this results in where clauses like this being added to compute a label:     <code>sql     where {table_date_key} &gt; {cut_date}     and {table_date_key} &lt; {cut_date} + interval '90 day'</code></li> <li>The label node is the table to run the label generation operation on.</li> <li>The label field is the field in the label node to run the operation on.</li> <li>The label operation is the actual operation to run, in this case count. In this example the label node, label field, and label operation tell us which field and aggregation primitive to run to compute the label:     <code>select customer_id,     count(id) as had_order_label     from 'orders.csv'     where {table_date_key} &gt; '5/1/2023'     and {table_date_key} &lt; '5/1/2023' + interval '90 day'     group by customer_id</code></li> <li>The execution does a depth first traversal through the graph, starting at the bottom and working it's way up to the parent node.  Each step of the way it applies date filters based on the cut date and performs aggregations prior to joining.  We'll dig more into how this works and how to customze compute graphs in the tutorial.</li> </ol>"}]}