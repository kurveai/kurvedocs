{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Kurve","text":"<p>Self-serve demo available at https://demo.kurve.ai</p>"},{"location":"#deployment","title":"Deployment","text":"<p>Kurve is deployed into your environment behind your VPC so no data ever leaves your network.</p>"},{"location":"#metadata-storage","title":"Metadata storage","text":"<p>The Kurve application is configured with <code>sqlite</code> by default but needs a relational database for the application-level models.  The metadata Kurve extracts and manages can also be optionally pushed directly into catalogs such as Unity Catalog.</p>"},{"location":"#authentication","title":"Authentication","text":"<p>Kurve supports basic auth and OAuth 2.0.  During a deployment, since Kurve runs within your cloud, there may be a manual configuration of the internal callback URL.</p> <p>An example might be a VPC-protected internal subdomain: <code>CUSTOMER.kurve.dev/callback</code></p>"},{"location":"#metadata-extraction-and-inference","title":"Metadata extraction and inference","text":"<p>Kurve providers a number of algorithms that point at data catalogs and infers the relational structure. The metadata Kurve automatically extracts are the following:</p>"},{"location":"#per-table","title":"Per table","text":"<ol> <li>primary key</li> <li>date key</li> <li>row count</li> </ol>"},{"location":"#per-table-pair","title":"Per table pair","text":"<ol> <li>join keys (if any)</li> <li>cardinality</li> </ol>"},{"location":"#metadata-graphs","title":"Metadata graphs","text":"<p>Kurve metadata graphs use graph data structures, powered by networkx under the hood, to represent tables as nodes and relationships between tables as edges.  This allows us to benefit from the whole field of graph theory and take advantage of the rich ecosystem around it.</p>"},{"location":"#compute-graphs","title":"Compute graphs","text":"<p>Metadata graphs are combined with compute graphs to run multi-table data integration.  Since the metadata are repesented in a graph it allows us to perform depth first search/traversal based on cardinality to integrate and aggregate data in a bottom up way (start at the high row count fact tables and aggregate upward).</p>"},{"location":"#quickstart-demo","title":"Quickstart demo","text":"<ol> <li>Create an account on demo.kurve.ai</li> <li> <p>Build a metadata graph on the sample data source <code>/usr/local/lake/cust_data</code>.</p> <ul> <li>this will infer the primary keys, date keys, and foreign keys between the tables</li> </ul> <p></p> </li> <li> <p>Inspect the metadata graph to confirm the relationships are correct.</p> <ul> <li>if there are incorrect relationships you can easily remove them </li> <li>if edges were missed they can be added</li> </ul> <p></p> </li> <li> <p>Build a compute graph with the <code>cust.csv</code> as the parent node using the following parameters:</p> <ol> <li>name: customer sample test</li> <li>parent node: <code>cust.csv</code></li> <li>depth limit: 2</li> <li>compute period in days: 365</li> <li>cut date: 5/1/2023</li> <li>label period in days: 60</li> <li>label node: <code>orders.csv</code></li> <li>label field: <code>id</code></li> <li>label operation: count</li> </ol> <p></p> </li> <li> <p>You should now see a compute graph with the <code>cust.csv</code> at the top and the <code>orders.csv</code> colored yellow since it is the target node.</p> </li> <li>In the compute graph viewier click on Actions and then Execute</li> <li>Navigate to the home screen and you should see a data source under My data sources</li> <li>Click on the data source and then click on the table name that was created which should    be the lower cased and underscore separated name you used for the compute graph.     </li> </ol>"},{"location":"#what-just-happened","title":"What just happened?","text":"<ol> <li> <p>The parent node indicates which table everything will be integrated to, in this case the <code>cust.csv</code>.</p> </li> <li> <p>The depth limit indicates how many joins away from the parent node are permissible, in this case tables within 2 joins are included.</p> </li> <li> <p>The cut date is the date around which to filter - so nothing after the cut date will be included in the data aggregation / integration process.</p> </li> <li> <p>The compute period indicates how far backwards relative to the cut date to look in all of the tables.</p> </li> <li> <p>The label period in days is how many days after the cut date to look for computing the label.</p> </li> <li> <p>The label node is the table to run the label generation operation on.</p> </li> <li> <p>The label field is the field in the label node to run the operation on.</p> </li> <li> <p>The label operation is the actual operation to run, in this case count.</p> </li> <li> <p>The execution does a depth first traversal through the graph, starting at the bottom and working it's way up to the parent node.  Each step of the way it applies date filters based on the cut date and performs aggregations prior to joining.  We'll dig more into how this works and how to customze compute graphs in the tutorial.</p> </li> </ol>"},{"location":"concepts/","title":"Abstractions","text":""},{"location":"concepts/#data-sources","title":"Data sources","text":"<p>A source of data, such as a Databricks catalog, a Redshift database and schema, or a Snowflake database and schema.  Data sources can be one or multiple schemas / namespaces.  In the Kurve UI they are represented as:</p> <p><code>{provider}://{tenant}/{namespace}?schema={schema}&amp;format={storage_format}</code></p> <p>For an S3 bucket called <code>bazooka</code> and a directory called <code>arms-sales</code> with parquet format we would have the following URI:</p> <p><code>s3://bazooka/arms-sales?format=parquet</code></p> <p>For a Snowflake account with a database called <code>bazooka</code>, and a schema called <code>arms-sales</code> we would have the following URI:</p> <p><code>snowflake://SOME-SNOWFLAKE-ACCOUNT/bazooka?schema=arms-sales&amp;format=relational</code></p>"},{"location":"concepts/#schema-graphs","title":"Schema graphs","text":"<p>A metadata representation of a data source which represents the tables or files as nodes and relationships, foreign keys, as edges.</p>"},{"location":"concepts/#nodes","title":"Nodes","text":"<p>Each node represents a single table or file.</p> <p>An example might be a table called <code>bazooka_sales</code>.</p>"},{"location":"concepts/#edges","title":"Edges","text":"<p>Edges define the foreign key between two nodes.</p> <p>An example would be a table called <code>bazooka_sales</code> and another table called <code>arms_dealers</code>.  The two tables, represented as nodes, are connected by the columns: <code>bazooka_sales.arms_dealer_id</code> which points to <code>arms_dealers.id</code>.</p> <p>This relationship is captured in the edge between <code>bazooka_sales</code> and <code>arms_dealers</code>.</p>"},{"location":"concepts/#compute-graphs","title":"Compute graphs","text":"<p>A compute graph is a subgraph of a schema graph and defines computation over the tables and relationships involved.</p> <p>The top-level compute graph houses the following parameters which are shared across all nodes:   - parent node   - cut date the compute period   - compute period   - label period   - label node   - label field   - label operation</p>"},{"location":"concepts/#compute-graph-execution-order-of-operations","title":"Compute graph execution order of operations","text":"<ol> <li>load and prefix data</li> <li>annotations</li> <li>filters</li> <li>reduce / aggregate<ol> <li>auto-apply time filters based on cut date, date key, and compute period</li> <li>apply reduce/aggregation operations</li> </ol> </li> <li>compute labels (if any)<ol> <li>auto-apply time filters based on cut date, date key, and label period</li> </ol> </li> <li>post-join annotations<ol> <li>annotations to run which require a child/foreign relation to be merged prior to running</li> </ol> </li> <li>post-join filters<ol> <li>filters to run which require a child/foreign relation to be merged prior to running</li> </ol> </li> </ol>"},{"location":"concepts/#compute-graph-nodes","title":"Compute graph nodes","text":"<p>Compute graph nodes allow us to define the following:</p> <ul> <li>the prefix to use for columns</li> <li>the date key to use (if any)</li> <li>the columns to include / exclude</li> <li>annotations</li> <li>filters</li> <li>reduce / aggregation operations</li> <li>labels / target generation operations for ML</li> <li>post join annotations</li> <li>post join filters</li> </ul>"},{"location":"concepts/#annotations","title":"annotations","text":"<p>Annotations are any manipulation of existing columns to create new ones such as applying a length function to a string column, extracting json to add new columns, or computing some function over a numerical column in a non-aggregated way.</p> <p>In plain english we're widening the table by adding new columns.  Let's say we have a table with 2 columns: <code>id INT</code> and <code>name INT</code>.  Kurve takes care of compiling the end to end SQL at runtime we can simply add the select and not think about the rest of the query:</p> <pre><code>select *, length(name) as name_length\n</code></pre> <p>At runtime this will compile into a full query like:</p> <pre><code>create some_stage_name as\nselect *, length(name) as name_length\nfrom table\n</code></pre> <p>We added the <code>name_length</code> column which is derived from the <code>name</code> column, so now we have 3 columns instead of 2.</p>"},{"location":"concepts/#filters","title":"filters","text":"<p>Filters are any filter applied to the node.  Since order of operation matters, they can also be combined with annotations.  They can be written in isolation without the rest of SQL grammar:</p> <pre><code>where lower(name) not like '%test%'\n</code></pre> <p>At runtime this will compile into a full query like:</p> <pre><code>select *\nfrom some_stage_name\nwhere lower(name) like '%test%'\n</code></pre>"},{"location":"concepts/#reduce-aggregate","title":"reduce / aggregate","text":"<p>Aggregations are any <code>group by</code> applied to the table prior to joining it to it's parent.  Kurve uses completely automated reduce operations powered by graphreduce but also allows full customizability.</p> <p>As with filters and annotations, reduce operations can be written in isolation without the where clauses and annotations as follows:</p> <pre><code>select customer_id,\ncount(*) as num_orders\nfrom 'orders.csv'\ngroup by customer_id\n</code></pre>"},{"location":"concepts/#labels-target-variables","title":"labels / target variables","text":""},{"location":"concepts/#post-join-annotations","title":"post-join annotations","text":""},{"location":"concepts/#post-join-filters","title":"post-join filters","text":""},{"location":"concepts/#compute-graph-edges","title":"Compute graph edges","text":""},{"location":"concepts/#compute-graph-decomposition","title":"Compute graph decomposition","text":""},{"location":"concepts/#thinking-at-the-node-level","title":"Thinking at the node-level","text":""},{"location":"concepts/#thinking-at-the-neighborhood-level","title":"Thinking at the neighborhood-level","text":""},{"location":"concepts/#how-compute-graphs-get-executed","title":"How compute graphs get executed","text":""},{"location":"concepts/#compute-pushdown","title":"Compute pushdown","text":"<p>As of writing, Kurve supports connectors to the following.</p> <ul> <li>amazon s3</li> <li>snowflake</li> <li>databricks</li> <li>amazon Redshift</li> <li>unity catalog</li> <li>polaris catalog</li> <li>amazon RDS</li> </ul>"},{"location":"concepts/#depth-first-traversal","title":"Depth-first traversal","text":""},{"location":"concepts/#dynamic-sql-generation-and-prefixing","title":"Dynamic SQL generation and prefixing","text":""},{"location":"concepts/#temporary-references-and-views","title":"Temporary references and views","text":""},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#relational-schema-inference-basics","title":"Relational schema inference basics","text":"<p>This tutorial covers the basic features of extracting relational metadata from flat files.</p> <ol> <li>Log in to kurve demo</li> <li>Look under Sample data sources for <code>/usr/local/lake/cust_data</code> and click Create Graph</li> <li>In a few seconds the page should refresh view the graph with View Graph</li> <li>View all of the foreign key relationships between the 6 tables:<ol> <li><code>cust.csv</code> connects with <code>orders.csv</code> on <code>id -&gt; customer_id</code></li> <li><code>cust.csv</code> connects with <code>notifications.csv</code> on <code>id -&gt; customer_id</code></li> <li><code>orders.csv</code> connects with <code>order_products.csv</code> on <code>id -&gt; order_id</code></li> <li><code>notifications.csv</code> connects with <code>notification_interactions.csv</code> on <code>id -&gt; notification_id</code></li> <li><code>notification_interactions.csv</code> connects with <code>notification_interaction_types.csv</code> on <code>interaction_type_id -&gt; id</code></li> </ol> </li> </ol> <p>In this example Kurve points at the 6 csv files, runs an algorithm, and extracts all of the relational metadata. Next we'll look at how to run computation on these relational metadata graphs.</p>"},{"location":"examples/#orienting-schema-graphs-around-particular-nodes","title":"Orienting schema graphs around particular nodes","text":"<p>Most analytics and AI problems are oriented around a particular dimension.  With a graph structure we can easily manipuate the schema graph around the dimension of interest.  For this example we'll orient the <code>/usr/local/lake/cust_data</code> sample schema around the <code>cust.csv</code>.  To do this we'll assign the <code>cust.csv</code> as the parent node:</p> <ol> <li>Under Actions in the schema graph viewer click Assign Parent Node and select <code>cust.csv</code> with <code>depth=1</code>.</li> <li>You should have a subgraph of 3 tables now: <code>cust.csv</code>, <code>orders.csv</code>, and <code>notifications.csv</code></li> <li>Now that we've oriented the dataset around the <code>cust.csv</code> dimension let's get to compute / data integration. </li> </ol>"},{"location":"examples/#compute-graph-basics","title":"Compute graph basics","text":"<ol> <li>Visit the schema graph for <code>/usr/local/lake/cust_data</code>.</li> <li>Click Actions, Assign Parent Node, select <code>cust.csv</code> with <code>depth = 1</code>.</li> <li>Click Actions, Compute Graph and plug in the following values:<ol> <li>Name: kurve cust demo</li> <li>Parent Node: cust.csv</li> <li>Depth limit: 1</li> <li>Compute period in days: 365</li> <li>Cut date: 05/01/2023</li> <li>Label period in days: 90</li> <li>Label node: notifications.csv</li> <li>Label field: id</li> <li>Label operation: count </li> </ol> </li> <li>Notice the color coating of the parent node, cust.csv, and the label node, notifications.csv.</li> <li>Let's make sure our parameters are correct: click Actions, Show compute graph details.  Here is what it should look like: </li> <li>If all of that looks good now let's execute the compute graph:<ol> <li>click Actions</li> <li>click Execute compute graph</li> <li>navigate home</li> <li>you should now see a data source under My data sources, click it and find the <code>kurve_cust_demo_train</code> table or whatever name you used and click on the table</li> <li>notice all of the columns but we should only have 4 rows, this is all 3 tables aggregated and integrated to the <code>cust.csv</code> dimension with point-in-time correctness based on the above parameters. </li> </ol> </li> </ol>"},{"location":"examples/#node-level-operations-customization-basic","title":"Node-level operations customization basic","text":"<p>In the compute graph created in the above example we leaned into Kurve's automation, but let's customize some things and see the effect.</p> <ol> <li> <p>Visit the compute graph under <code>/usr/local/lake/cust_data</code></p> </li> <li> <p>Click the parent node <code>cust.csv</code></p> </li> <li> <p>Click Edit Node and under Annotations / special selects enter the following:</p> <p><code>select *, length(name) as name_length</code></p> </li> <li> <p>Under Filters enter the following:</p> <p><code>where name_length &lt; 4</code>  5. Execute the compute graph 6. Navigate home and click on the compute graph output table under My data sources and you shuld now see only 3 rows. </p> </li> </ol> <p>In summary, we annotated the <code>cust.csv</code> node with a new column called <code>name_length</code> and then filtered the dataset to only contain rows where <code>name_length &lt; 4</code>, which filtered one row.  This highlights how we can customize compute graphs in a basic way.  The next example will do this in a more advanced way.</p>"},{"location":"examples/#node-level-operations-customization-advanced","title":"Node-level operations customization advanced","text":"<p>Continuing with the same schema and compute graph from the prior 2 examples let's say we need to customize some of the aggregations on the <code>orders.csv</code> table and perform some filters on that table, as well.</p> <ol> <li>Visit the compute graph from the prior example</li> <li>Click on the <code>orders.csv</code> node</li> <li> <p>Click Edit Node and under Filters enter the following:</p> <p><code>where total &lt; 2000</code></p> </li> </ol>"}]}